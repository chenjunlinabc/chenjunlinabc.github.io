<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>python爬虫学习笔记 | 知政的个人博客</title>
<meta name=keywords content="Python"><meta name=description content="urllib，xpath，jsonpath，beautiful，requests，selenium，Scrapy
python库内置的HTTP请求库 urllib.request 请求模块 urllib.error 异常处理模块 urllib.parse url解析模块 urllib.robotparsef robots.txt解析模块
urllib.request提供了最基本的http请求方法，主要带有处理授权验证，重定向，浏览器Cookies功能
模拟浏览器发送get请求，就需要使用request对象，在该对象添加http头 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(response.read().decode(&lsquo;utf-8&rsquo;))
使用type()方法 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(type(response))
HTTPResposne类型对象
通过status属性获取返回的状态码 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(response.status) print(response.getheaders())
post发送一个请求，只需要把参数data以bytes类型传入 import urllib.parse import urllib.request data = bytes(urllib.parse.urlencode({&lsquo;hallo&rsquo;:&lsquo;python&rsquo;}),encoding=&lsquo;utf-8&rsquo;) response = urllib.request.urlopen(&lsquo;http://httpbin.org/post'.data = data) print(response.read())
timeout参数用于设置超时时间，单位为秒 import urllib.request response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/',timeout=1)
这里设置超时时间为1秒，如果超了1秒，服务器依然没有响应就抛出URLError异常，可以结合try和except
import urllib.parseimport urllib.requesturl = &#34;https://xiaochenabc123.test.com/&#34;headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537."><meta name=author content="Me"><link rel=canonical href=https://99999.fun/posts/125/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.4599eadb9eb2ad3d0a8d6827b41a8fda8f2f4af226b63466c09c5fddbc8706b7.css rel="preload stylesheet" as=style><link rel=icon href=https://99999.fun/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://99999.fun/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://99999.fun/favicon.ico><link rel=apple-touch-icon href=https://99999.fun/favicon.ico><link rel=mask-icon href=https://99999.fun/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://99999.fun/posts/125/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="python爬虫学习笔记"><meta property="og:description" content="urllib，xpath，jsonpath，beautiful，requests，selenium，Scrapy
python库内置的HTTP请求库 urllib.request 请求模块 urllib.error 异常处理模块 urllib.parse url解析模块 urllib.robotparsef robots.txt解析模块
urllib.request提供了最基本的http请求方法，主要带有处理授权验证，重定向，浏览器Cookies功能
模拟浏览器发送get请求，就需要使用request对象，在该对象添加http头 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(response.read().decode(&lsquo;utf-8&rsquo;))
使用type()方法 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(type(response))
HTTPResposne类型对象
通过status属性获取返回的状态码 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(response.status) print(response.getheaders())
post发送一个请求，只需要把参数data以bytes类型传入 import urllib.parse import urllib.request data = bytes(urllib.parse.urlencode({&lsquo;hallo&rsquo;:&lsquo;python&rsquo;}),encoding=&lsquo;utf-8&rsquo;) response = urllib.request.urlopen(&lsquo;http://httpbin.org/post'.data = data) print(response.read())
timeout参数用于设置超时时间，单位为秒 import urllib.request response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/',timeout=1)
这里设置超时时间为1秒，如果超了1秒，服务器依然没有响应就抛出URLError异常，可以结合try和except
import urllib.parseimport urllib.requesturl = &#34;https://xiaochenabc123.test.com/&#34;headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537."><meta property="og:type" content="article"><meta property="og:url" content="https://99999.fun/posts/125/"><meta property="og:image" content="https://99999.fun/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-12-31T01:05:00+00:00"><meta property="article:modified_time" content="2021-12-31T01:05:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://99999.fun/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="python爬虫学习笔记"><meta name=twitter:description content="urllib，xpath，jsonpath，beautiful，requests，selenium，Scrapy
python库内置的HTTP请求库 urllib.request 请求模块 urllib.error 异常处理模块 urllib.parse url解析模块 urllib.robotparsef robots.txt解析模块
urllib.request提供了最基本的http请求方法，主要带有处理授权验证，重定向，浏览器Cookies功能
模拟浏览器发送get请求，就需要使用request对象，在该对象添加http头 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(response.read().decode(&lsquo;utf-8&rsquo;))
使用type()方法 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(type(response))
HTTPResposne类型对象
通过status属性获取返回的状态码 import urllib.requst response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/') print(response.status) print(response.getheaders())
post发送一个请求，只需要把参数data以bytes类型传入 import urllib.parse import urllib.request data = bytes(urllib.parse.urlencode({&lsquo;hallo&rsquo;:&lsquo;python&rsquo;}),encoding=&lsquo;utf-8&rsquo;) response = urllib.request.urlopen(&lsquo;http://httpbin.org/post'.data = data) print(response.read())
timeout参数用于设置超时时间，单位为秒 import urllib.request response = urllib.request.urlopen(&lsquo;https://xiaochenabc123.test.com/',timeout=1)
这里设置超时时间为1秒，如果超了1秒，服务器依然没有响应就抛出URLError异常，可以结合try和except
import urllib.parseimport urllib.requesturl = &#34;https://xiaochenabc123.test.com/&#34;headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://99999.fun/posts/"},{"@type":"ListItem","position":2,"name":"python爬虫学习笔记","item":"https://99999.fun/posts/125/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"python爬虫学习笔记","name":"python爬虫学习笔记","description":"urllib，xpath，jsonpath，beautiful，requests，selenium，Scrapy\npython库内置的HTTP请求库 urllib.request 请求模块 urllib.error 异常处理模块 urllib.parse url解析模块 urllib.robotparsef robots.txt解析模块\nurllib.request提供了最基本的http请求方法，主要带有处理授权验证，重定向，浏览器Cookies功能\n模拟浏览器发送get请求，就需要使用request对象，在该对象添加http头 import urllib.requst response = urllib.request.urlopen(\u0026lsquo;https://xiaochenabc123.test.com/') print(response.read().decode(\u0026lsquo;utf-8\u0026rsquo;))\n使用type()方法 import urllib.requst response = urllib.request.urlopen(\u0026lsquo;https://xiaochenabc123.test.com/') print(type(response))\nHTTPResposne类型对象\n通过status属性获取返回的状态码 import urllib.requst response = urllib.request.urlopen(\u0026lsquo;https://xiaochenabc123.test.com/') print(response.status) print(response.getheaders())\npost发送一个请求，只需要把参数data以bytes类型传入 import urllib.parse import urllib.request data = bytes(urllib.parse.urlencode({\u0026lsquo;hallo\u0026rsquo;:\u0026lsquo;python\u0026rsquo;}),encoding=\u0026lsquo;utf-8\u0026rsquo;) response = urllib.request.urlopen(\u0026lsquo;http://httpbin.org/post'.data = data) print(response.read())\ntimeout参数用于设置超时时间，单位为秒 import urllib.request response = urllib.request.urlopen(\u0026lsquo;https://xiaochenabc123.test.com/',timeout=1)\n这里设置超时时间为1秒，如果超了1秒，服务器依然没有响应就抛出URLError异常，可以结合try和except\nimport urllib.parse\rimport urllib.request\rurl = \u0026quot;https://xiaochenabc123.test.com/\u0026quot;\rheaders = {\r'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.","keywords":["Python"],"articleBody":"urllib，xpath，jsonpath，beautiful，requests，selenium，Scrapy\npython库内置的HTTP请求库 urllib.request 请求模块 urllib.error 异常处理模块 urllib.parse url解析模块 urllib.robotparsef robots.txt解析模块\nurllib.request提供了最基本的http请求方法，主要带有处理授权验证，重定向，浏览器Cookies功能\n模拟浏览器发送get请求，就需要使用request对象，在该对象添加http头 import urllib.requst response = urllib.request.urlopen(‘https://xiaochenabc123.test.com/') print(response.read().decode(‘utf-8’))\n使用type()方法 import urllib.requst response = urllib.request.urlopen(‘https://xiaochenabc123.test.com/') print(type(response))\nHTTPResposne类型对象\n通过status属性获取返回的状态码 import urllib.requst response = urllib.request.urlopen(‘https://xiaochenabc123.test.com/') print(response.status) print(response.getheaders())\npost发送一个请求，只需要把参数data以bytes类型传入 import urllib.parse import urllib.request data = bytes(urllib.parse.urlencode({‘hallo’:‘python’}),encoding=‘utf-8’) response = urllib.request.urlopen(‘http://httpbin.org/post'.data = data) print(response.read())\ntimeout参数用于设置超时时间，单位为秒 import urllib.request response = urllib.request.urlopen(‘https://xiaochenabc123.test.com/',timeout=1)\n这里设置超时时间为1秒，如果超了1秒，服务器依然没有响应就抛出URLError异常，可以结合try和except\nimport urllib.parse\rimport urllib.request\rurl = \"https://xiaochenabc123.test.com/\"\rheaders = {\r'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'}\rdata = urllib.request.Request(url=url, headers=headers)\rhi = hallo = urllib.request.urlopen(data)\r#hallo = urllib.request.urlopen(data).read().decode('utf-8')\r#print(hallo)\r# 6个方法\r#abc = hi.read(6) #按字节返回\r#xyz = hi.readline() # 读取一行\r#a = hi.readlines() # 一行一行的读取\r#b = hi.getcode() # 状态码\r#c = hi.geturl() # 获取url地址\r#d = hi.getheaders() # 获取状态信息\r# 下载到本地\r#urllib.request.urlretrieve(url,data.html)\r#url_img = \"https://xiaochenabc123.test.com/1.jpg\"\r#urllib.request.urlretrieve(url_img,abc.jpg)\r#url_mp4 = \"https://xiaochenabc123.test.com/1.mp4\"\r#urllib.request.urlretrieve(url_img,xyz.mp4)\r# auote() 转Unicode编码\r#name = urllib.parse.quote(\"哈哈哈\")\r#urlencode(),get拼接\rgg = {\r\"name\": \"小陈\",\r\"data\": \"hallo word\"\r}\rhaha = urllib.parse.urlencode(gg)\r#print(haha)\r# post请求\rssr = {\rdata: \"abc\"\r}\rabcxyz = urllib.parse.urlencode(ssr).encode('utf‐8')\rnogo = urllib.request.Request(url=url, headers=headers, data=abcxyz)\ryesgo = urllib.request.urlopen(nogo)\r#print(yesgo.read().decode('utf‐8'))\r#ajax请求（get和post）\r#cookie（扩展headers）\rheaders = {\r'Cookie': 'xxxx'\r}\r#HTTPHandler()\r#request = urllib.request.Request(url=url, headers=headers) #handler = urllib.request.HTTPHandler()\r#opener = urllib.request.build_opener(handler) #response = opener.open(request) #print(response.read().decode('utf‐8'))\r#代理服务器\rurls = \"http://baidu.com\"\rrequest = urllib.request.Request(url=urls, headers=headers)\rproxies = {'https': '14.115.106.223:808'}\rhandler = urllib.request.ProxyHandler(proxies=proxies) opener = urllib.request.build_opener(handler) response = opener.open(request) content = response.read().decode('utf‐8')\r#print(content)\rfrom lxml import etree\rimport urllib.request\r# 解析本地文件\r#html_data = etree.parse('./hallo.html', etree.HTMLParser())\r# 解析服务器响应文件\r# html = etree.HTML(response.read().decode('utf‐8')\r#result = etree.tostring(html_data)\r# print(result.decode('utf-8'))\r# /为查找子节点，//为查找全部子孙节点（不考虑层级）\r# list = html_data.xpath(\"//ul/li\")\r# 查找全部带id\r# list = html_data.xpath(\"//ul/li[@id]/text()\")\r# 查找指定id\r# list = html_data.xpath('//ul/li[@id=\"a\"]/text()')\r# 查找指定class的\r# list = html_data.xpath('//ul/li[@class=\"a\"]/text()')\r# 模糊查询（包含）\r# list = html_data.xpath('//ul/li[contains(@id, \"ab\")]/text()')\r# 查找以什么开头的\r# list = html_data.xpath('//li[starts-with(@id, \"a\")]/text()')\r# 查找以什么结尾的\r# list = html_data.xpath('//li[ends-with(@id, \"a\")]/text()')\r# 查找内容\r# list = html_data.xpath('//a[text()=\"小陈的辣鸡屋\"]/text()')\r# 多属性匹配(和)\r# list = html_data.xpath('//li[contains(@id, \"a\") and @href=\"https://xiaochenabc123.test.com\"]/a/text()')\r# 或者\r# list = html_data.xpath('//li[contains(@id, \"a\") | @href=\"https://xiaochenabc123.test.com\"]/a/text()')\r# 查找指定顺序的\r# list = html_data.xpath('//a[1]/text()') #获取第一个，可以指定第几个\r# list = html_data.xpath('//a[last()]/text()') #获取最后一个\r# list = html_data.xpath('//a[position()\u003c6]/text()') # 获取前5个\r# list = html_data.xpath('//a[last()-3]/text()') #获取倒数第4个\r# print(list)\r#实例：获取logo和logo的url\r#url = \"https://xiaochenabc123.test.com\"\r#headers = {\r# 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'\r#}\r#request = urllib.request.Request(url=url, headers=headers)\r#response = urllib.request.urlopen(request)\r#content = response.read().decode(\"utf-8\")\r#hallo = etree.HTML(content)\r#data = hallo.xpath('//h3[@id=\"logo\"]/a/@href')[0]\r#data1 = hallo.xpath('//h3[@id=\"logo\"]/a/text()')[0]\r#print(data,data1)\r#获取文章的标题\r#headers = {\r# 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'\r#}\r#\r#if __name__ == \"__main__\":\r# page1 = int(input(\"开始\"))\r# page2 = int(input(\"结束\"))\r#\r# for page in range(page1, page2 + 1):\r# urldata = \"https://xiaochenabc123.test.com/archives/\" + str(page) + \".html\"\r# #print(urldata)\r# request = urllib.request.Request(url=urldata, headers=headers)\r# response = urllib.request.urlopen(request)\r# if (response.getcode() == 200):\r# content = response.read().decode(\"utf-8\")\r# hallo = etree.HTML(content)\r# data = hallo.xpath('//h3[@class=\"post-title\"]/a/text()')[0]\r# print(data)\r# elif(response.getcode() == 404):\r# continue\r#爬取网站的全部a链接，并且返回到数组中\r#if __name__ == \"__main__\":\r# url = input(\"请输入要爬取的url\")\r# abc = int(input(\"是否启动只搜索当前域名的url,请回复0或者1\"))\r# headers = {\r# 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'\r# }\r# request = urllib.request.Request(url=url, headers=headers)\r# response = urllib.request.urlopen(request)\r# content = response.read().decode(\"utf-8\")\r# hallo = etree.HTML(content)\r# data = hallo.xpath('//a/@href')\r# a = 1\r# b = 1\r# c = \"https\"\r# d = \"http\"\r# if (abc == 1):\r# while a \u003c len(data):\r# if (c or d in data[a]):\r# if (url in data[a]):\r# urldata = data[a]\r# print(urldata)\r# a += 1\r# else:\r# while b \u003c len(data):\r# if (c or d in data[a]):\r# urldata = data[b]\r# print(urldata)\r# b += 1\r练习（json解析bing官方提供的api，获取精密壁纸）\nimport jsonpath\rimport json\rimport urllib.request\r# pip install jsonpath\rheaders = {\r'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'\r}\rabc = 0\rwhile abc == 0 :\ra = int(input(\"请输入要下载的张数（bing最多只能下载8张）\"))\raa = str(a)\rif(a\u003c=8):\rurlapi = \"https://cn.bing.com/HPImageArchive.aspx?format=js\u0026idx=0\u0026n=\"+aa+\"\u0026mkt=zh-CN\"\rrequest = urllib.request.Request(url=urlapi, headers=headers)\rresp = urllib.request.urlopen(request)\rcontent = resp.read().decode(\"utf-8\")\rwith open(\"data.json\", \"w\", encoding=\"utf-8\") as fp:\rfp.write(content)\robj = json.load(open(\"data.json\", \"r\", encoding=\"utf-8\"))\rhallo = jsonpath.jsonpath(obj, \"$..url\")\rimg = jsonpath.jsonpath(obj, \"$..enddate\")\rc = 0\rb = 0\rwhile c \u003c len(hallo):\rhi = \"https://cn.bing.com\"\rdata = hi + hallo[c]\rprint(data)\rc+=1\rwhile b \u003c len(img):\rhaha = img[b] + \".jpg\"\rprint(haha)\rurllib.request.urlretrieve(data, filename=haha)\rb+=1\rbreak\rprint(\"下载完成！！！\")\rabc = 1\relse:\rprint(\"您输入的张数超过8张，bing官方api只能下载8张，请重新输入\")\rBeautiful Soup解析库，和lxml一样，可以从html或者xml中解析数据和提取数据，Beautiful Soup会自动将输入数据转换为unicode编码，输出数据转换为utf-8编码。不需要考虑编码问题。除非文档没有说明编码\n目前使用的是Beautiful Soup 4，简称bs4，Beautiful Soup3已停止维护\n安装\npip install beautifulsoup4\n注意：Beautiful Soup支持第三方解析器，如果不使用第三方解析器的话，将使用Python标准库中的html解析器\nBeautifulSoup(html, html.parser) # 内置标准库\rBeautifulSoup(html, lxml) # lxml库，需要下载lxml库\rBeautifulSoup(html, xml) # lxml的xml解析，需要下载lxml库\rBeautifulSoup(html, html5lib) # html5lib，需要下载html5lib库 pip install html5lib\r第一个例子\nfrom bs4 import BeautifulSoup\r# data = BeautifulSoup(open(\"haha.html\"),encoding=\"utf-8\", \"lxml\")\rdata = BeautifulSoup(open(\"haha.html\"))\rprint(data.prettify())\rbs会将html文档解析为树状结构，该树状结构的节点是Python对象，而这些对象可以分为4种：\nTag：标签，通过tag获取指定标签内容，print(data.div)，可以通过data.标签名的方式获取标签的内容（注意：输出第一个符合条件的标签）\n检查对象的类型：print (type(data.div))，可以看到输出结果为，说明该对象为tag\ntag有两个属性，分别为name和attrs\nprint (data.name)\rprint (data.div.name)\r可以看到输出结果为[document]和div，data的name为[document]，而标签输出为标签本身的名字\nprint (data.div.attrs)\r可以看到输出结果是{‘class’: [‘xxx’]}的键值对，可以通过data.div[“xxx”]方式获取属性值，也可以修改属性值（data.div[“class”]= “nav”），删除属性（del data.div[“class”]）\nNavigableString：标签内部的文本（.string），print (data.p.string)，判断类型，print (type(data.p.string))，输出结果为\nBeautifulSoup：文档的全部内容，也就是data本身，print (type(data))，\nComment：特殊一点的NavigableString类型，可以输出注释的内容（不带注释符号），通过类型判断print (type(data.span.string))，可以看到输出\n查找\nprint(data.div.content) # 查找div的全部子节点，并且以列表的方式输出，可以使用索引来指定获取第几个节点\rprint(data.div.children) #一样是查找div的全部子节点，不过是list生成器，需要手动遍历获取\rprint(data.select(\"span\")) #查找标签，也可以通过类名或者id名查找，支持组合查找（和css方式一样）\rprint(data.find(\"div\"))\rprint(data.find(\"div\",class_=\"nav\"))\rselenium是一个web应用自动化测试工具，selenium可以直接运行在目前主流浏览器驱动（本身没有浏览器功能，需要搭配第三方浏览器，支持无界面浏览器），模拟用户操作\n因为其自动化和可以直接运行在浏览器的原因，可以用于爬虫\nchromedriver：https://npm.taobao.org/mirrors/chromedriver（淘宝镜像地址，下载的驱动版本要和浏览器内核的版本一样） edge: https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/ （webdriver.Edge()） 火狐：https://github.com/mozilla/geckodriver/releases （webdriver.Firefox()）\n安装\npip install selenium\n导入驱动\nfrom selenium import webdriver\rpath = \"浏览器驱动路径\"\rdriver = webdriver.Chrome(path)\rurl = \"https://xiaochenabc123.test.com\"\rdriver.get(url)\rdata = driver.page_source\rprint(data)\r元素定位（模拟鼠标和键盘操作元素的点击或者输入等等，在操作元素之前需要获取到元素的位置）\n注意：element是返回单个对象，elements可以返回多个对象\n根据id查找元素\ndata = driver.find_element_by_id(“app”)\n根据name属性值查找 data = driver.find_element_by_name(\"\")\n根据xpath查找 data = driver.find_elements_by_xpath()\n根据标签名查找 data = driver.find_elements_by_tag_name()\n使用bs4的方式查找 data = driver.find_element_by_css_selector(\"#app\")\n根据连接的文本查找 data = driver.find_element_by_link_text()\n根据连接的文本查找（模糊） data = driver.find_element_by_partial_link_text()\n根据class名查找 data = driver.find_element_by_class_name()\n交互和数据获取\n数据获取\n获取属性值\nprint(data.get_attribute(“href”))\n获取标签名\nprint(data.tag_name)\n获取元素文本\nprint(data.text)\n交互\n指定浏览器大小\ndriver.set_window_size(900, 1000)\n浏览器的前进和后退，以及刷新\ndriver.forward() driver.back() driver.refresh()\ninput输入框操作\ndata.click() #单击元素 data.send_keys(“hallo”) #输入 data.clear() #清除内容\n执行指定js脚本\ndriver.execute_script(js)\n例如：\nimport time\rfrom selenium import webdriver\rpath = \"msedgedriver.exe\"\rdriver = webdriver.Edge(path)\rurl = \"https://www.baidu.com/\"\rdriver.get(url)\rdata = driver.page_source\rdatas = driver.find_element_by_id(\"kw\")\rdatas.send_keys(\"小陈的辣鸡屋\")\ra = driver.find_element_by_id(\"su\")\ra.click()\rtime.sleep(2)\r#bottom = 'document.documentElement.scrollTop=10000'\r#driver.execute_script(bottom)\rb = driver.find_element_by_link_text(\"小陈的辣鸡屋\")\rb.click()\rPhantomjs是一个无界面浏览器，因为其不需要进行gui渲染，效率比较高，但是已经停止更新，这里只了解一下，推荐使用Headless Chrome\nhttps://phantomjs.org/\nphantomjs操作方式一样\ndriver = webdriver.PhantomJS(path)\r因为没有界面，访问网页是没有界面的，需要通过快照获取，driver.save_screenshot(“xiaochenabc123.test.com.jpg”)\nHeadless Chrome是基于Chrome 59版本以及以上版本的无界面模式（mac和Linux最低要求59版本，而win需要60版本）\nfrom selenium import webdriver\rfrom selenium.webdriver.chrome.options import Options\rchrome_options = Options()\rchrome_options.add_argument('--headless') # 无界面模式启动\rchrome_options.add_argument('--disable-gpu')\rpath = \"Chrome.exe\" # Chrome浏览器的路径\rchrome_options.binary_location = path\rdriver = webdriver.Chrome(chrome_options=chrome_options) # 实例化\rurl = \"https://xiaochenabc123.test.com\"\rdriver.get(\"url\")\rdriver.save_screenshot(\"xiaochenabc123.test.com.jpg\")\r具体操作方法和selenium一样\nrequests库是基于Python开发并且封装的http库，HTTP for Humans\n安装\npip install requests\n使用方式很简单\nimport requests\rurl = \"https://xiaochenabc123.test.com\"\rresponse = requests.get(url)\rresponse.status_code # 200 返回状态码\rprint(type(response)) # 返回类型\rresponse.encoding = \"utf-8\" # 响应的编码\rprint(response.text) # 返回源代码\rprint(response.url) # 返回请求的url\rprint(response.content) # 返回二进制数据\rprint(response.headers) # 返回响应头\rrequests在请求json的时候还可以直接获取\nget请求\nget请求的参数通过params属性传递，不需要考虑url的编码问题，不需要考虑请求对象，例如：\ndata = {\r'wd': 'python'\r}\rheaders = {\r'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'\r}\rresponse = requests.get('https://baidu.com/s?', params=data, headers=headers)\rprint (response.text)\rpost请求\npost请求不需要考虑编解码，不需要考虑请求对象\ndata = {\r'url': 'xiaochenabc123.test.com',\r'name': 'root'\r}\rurl = \"https://httpbin.org/post\"\rresponse = requests.post(url=url, data=data, headers=headers)\rimport json\robj = json.loads(response.text,encoding=\"utf-8\")\rprint (obj)\r注意：默认使用application/x-www-form-urlencoded进行编码，如果想传递json，需要使用requests.post()的json参数，上传文件用files参数\n另外还支持put()和delete()\n传入cookie需要cookies参数，支持设置请求超时（timeout参数，单位为秒）\n代理用proxies参数，指向一个字典（代理池）\nscrapy是基于Python开发的爬取数据，提取数据的框架，可应用在数据挖掘，数据存储\n安装\npip install scrapy\nscrapy架构组件分为Scrapy Engine（引擎），Scheduler（调度器），Downloader（下载器），Spider（爬虫），Item Pipeline（数据管道），Downloader middlewares（下载中间件），Spider middlewares（Spider中间件）\nScrapy Engine：负责控制数据流（Data Flow）在组件中的流动，例如通信，数据传递，在某些情况下还触发相对应的事件\nScheduler：负责接收引擎发送的request请求并且将其存储在调度器中，当引擎需要时提供给引擎，调度器会自动去重url（也可以不去重）\nDownloader：负责下载引擎发送的全部request请求的响应数据，将获取到数据提供给Spider\nSpider：负责处理全部响应数据，并且进行分析提取数据，获取需要的数据，并且将需要进行额外跟进的url传递给引擎，以便重新进入调度器，一般情况一个Spider只负责一个或者多个特定的url\nItem Pipeline：负责处理Spider提取出来的数据，经过分析，过滤，存储等步骤，最后存储在本地或者数据库中\nDownloader middlewares：其是引擎和Downloader之间的钩子，自定义扩展下载器的功能，例如更换ua，ip等等\nSpider middlewares：Spider和引擎之间的钩子，用来处理Spider的输入响应和输出数据，自定义扩展Spider的功能\n具体数据流线路：引擎请求一个url，并且获取到处理这个url的Spider，并且向该Spider请求要第一个爬取的url，引擎获取到要爬取的url，将其传递给调度器，引擎再向调度器请求要下一个爬取的URL，调度器返回url给引擎，引擎通过下载中间件传递给下载器，由下载器下载数据，当数据下载完毕（不管是否成功），下载器将响应数据通过下载中间件返回给引擎，引擎得到数据并且通过Spider中间件传递给Spider处理数据，Spider处理数据并且将处理完毕的数据以及需要跟进的url提供给引擎，引擎再将处理完毕的数据传递给Item Pipeline进行下一步处理，并且将需要跟进的url传递给调度器，一直重复循环直到调度器中没有request\n新建项目：\nscrapy startproject scrapyDemo\n这个项目中包含了一些文件，分别是scrapy.cfg（配置文件），scrapyDemo/（项目的Python模块，将在这里编写程序），items.py（item文件，目标文件），pipelines.py（管道文件，用来处理数据，默认为300优先级，值越小优先级越高），settings.py（项目的设置文件，例如robots协议是否遵守，定义ua等等），spiders/（spider爬虫程序存放的目录）\nspider爬虫程序（/spiders目录下）\n创建一个爬虫必须继承scrapy.Spider类，并且定义三个参数（必须唯一，用来区别其他爬虫），start_urls（爬虫程序执行时要爬取的url），parse()在被调用时，下载完成后得到的响应将作为唯一参数传递给该方法，这个方法负责解析返回回来的数据，以及提取数据和后续要进行下一步处理的响应数据\n快速创建一个基础scrapy爬虫程序\nscrapy genspider cjlio xiaochenabc123.test.com\nimport scrapy\rclass CjlioSpider(scrapy.Spider):\rname = 'cjlio'\rallowed_domains = ['xiaochenabc123.test.com'] # 过滤爬取的URL，不在此范围内的域名会被过滤掉\rstart_urls = ['http://xiaochenabc123.test.com/']\rdef parse(self, response):\rcontent = response.text\rprint(content)\rresponse属性：.text（返回响应的字符串），.body（返回二进制数据），.xpath()（使用xpath方法来解析，返回的数据为列表），.extract()获取seletor对象的data属性值，.extract_first()获取seletor列表的第一个的数据\n还有.url（url地址），.status（状态码），.encoding（响应正文的编码），.request（生成request对象），.css()（用css选择器方式解析数据），.urljoin()（生成绝对url）\n执行爬虫程序（cjlio为爬虫的name）\nscrapy crawl cjlio\n将获取的数据存储为json文件\nscrapy crawl cjlio -o cjlio.json\n注意：因为scrapy默认使用ascii存储json，需要改为utf-8\n在settings.py中添加FEED_EXPORT_ENCODING = ‘utf-8’解决\n关闭robots协议（不遵守robots协议，注释掉或者改为False，默认遵守robots协议）\nROBOTSTXT_OBEY = False\rscrapy shell是一个交互终端，在没有启动spider的情况下尝试以及测试程序的xpath和css表达式\nscrapy shell可以借助ipython终端\npip install ipython\n注意：如果安装ipython，scrapy将使用ipython代替Python标准终端，ipython提供了自动补全，高亮等等功能\n调试例子（不需要进入Python环境，可以直接在终端中调试）\nscrapy shell xiaochenabc123.test.com\nresponse.text\nresponse.xpath()\nitems.py定义数据结构\nimport scrapy\rclass ScrapydemoItem(scrapy.Item):\rname = scrapy.Field()\rimg = scrapy.Field()\rcjlio.py（爬虫程序）\nimport scrapy\rclass CjlioSpider(scrapy.Spider):\rname = 'cjlio'\rallowed_domains = ['xiaochenabc123.test.com']\rstart_urls = ['http://xiaochenabc123.test.com/']\rdef parse(self, response):\rcontent = response.text\rname = content.xpath(\"xpath匹配\").extract_first()\rimg = content.xpath(\"xpath匹配\").extract_first()\rprint(name, img)\r管道封装\ncjlio.py（爬虫程序）\nfrom scrapyDemo.items import ScrapydemoItem\r...\rdata = scrapyDemoItems(name=name,img=img)\ryield data # 每获取一次data数据，就是返回一次数据给管道\r# yield的作用类似于生成器（generator）\r注意：如果想使用管道，需要在settings.py中开启\n找到\nITEM_PIPELINES = {\r'scrapyDemo.pipelines.ScrapydemoPipeline': 300,\r}\r将注释去掉，300指优先级（优先级1到1000），值越小优先级越高\npipelines.py（管道文件）中的item就是爬虫程序返回的data数据\n启动爬虫之前（open_spider(self, spider)）\n启动爬虫之后（close_spider(self, spider)）\n开启多管道\n在 ITEM_PIPELINES 字段中添加新管道（settings.py）\n然后在pipelines.py定义新管道的类\n查询当前项目中的爬虫任务：scrapy list\n自定义UA\nsettings.py文件中USER_AGENT字段或者DEFAULT_REQUEST_HEADERS都可以\n项目中配置headers\nspiders爬虫程序\nheaders = {\r'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'\r}\rurl = \"https://xiaochenabc123.test.com\"\ryield scrapy.Request(\rurl=url,\rheaders=headers\r)\r数据入库\n安装pymysql\npip install pymysql\nsettings.py\n（pymysql） DB_HOST = “127.0.0.1” DB_RORT = 3306 DB_USER = “root” DB_PASSWORD = “123abc” DB_NAME = “dataMax” DB_CHARSET = “utf8”\n开一条新管道，专门处理数据入数据库\npipeline.py\nfrom scrapy.utils.project import get_project_settings\rimport pymysql\r#读取settings文件\rclass MysqlMax:\rdef open_spider(self, spider):\rsettings = get_project_settings()\rself.host = settings['DB_HOST']\rself.port = settings['DB_PORT']\rself.user = settings['DB_USER']\rself.password = settings['DB_PASSWORD']\rself.name = settings['DB_NAME']\rself.charset = settings['DB_CHARSET']\rself.connect()\rdef connect(self):\rself.conn = pymysql.connect(\rhost = self.host,\rport = self.port,\ruser = self.user,\rpassword = self.password,\rname = self.name,\rcharset = self.charset\r)\rself.cursor = self.conn.cursor()\rdef process_item(self,item,spider):\rsql = 'insert into book(name,src) values(\"{}\",\"{}\").format(item['name'],item['img'])\rself.cursor.execute(sql)\rself.conn.commit()\r# 执行sql语句，并且提交\rreturn item\rdef close_spider(self, spider):\rself.cursor.close()\rself.conn.close()\r链接跟进和链接提取器\n爬虫程序文件\nrules = (\rRule(LinkExtractor(allow = \"\"),\rcallback = \"parse_item\", follow = True),\r)\rallow为正则表达式，当链接满足正则表达式提取，为空则全部匹配（链接提取器）\ncallback为指定解析数据的规则\nfollow一般情况默认为False，指定是否从response提取的链接进行跟进，当callback为none时，follow为True\n还有dent =()，用来过滤符合正则表达式的链接，当符合时不提取\nallow_domains：允许的域名，deny_domains：不允许的域名\nrestrict_xpaths：提取符合xpath的链接，restrict_css：提取符合选择器的链接\n注意：follow当为True会一直提取符合规则的链接，直到全部链接提取完毕\n日志以及日志等级\n日志等级\nCRITICAL：严重错误 ERROR：一般错误 WARNING：警告 INFO：一般信息 DEBUG：调试信息（默认，只有出现DEBUG以及以上等级的日志才会打印输出）\nsettings.py文件可以设置哪些日志显示，哪些不显示\nLOG_FILE：将信息存储到文件中，不显示在输出界面，后缀为.log\nLOG_LEVEL：指定日志显示的等级\n例如： LOG_LEVEL = “WARNING” LOG_FILE= “demo.log”\n","wordCount":"1414","inLanguage":"en","datePublished":"2021-12-31T01:05:00Z","dateModified":"2021-12-31T01:05:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://99999.fun/posts/125/"},"publisher":{"@type":"Organization","name":"知政的个人博客","logo":{"@type":"ImageObject","url":"https://99999.fun/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://99999.fun/ accesskey=h title="知政的个人博客 (Alt + H)"><img src=https://99999.fun/favicon.ico alt aria-label=logo height=35>知政的个人博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://99999.fun/ title=首页><span>首页</span></a></li><li><a href=https://99999.fun/categories/ title=分类><span>分类</span></a></li><li><a href=https://99999.fun/archives/ title=归档><span>归档</span></a></li><li><a href=https://99999.fun/tags/ title=标签><span>标签</span></a></li><li><a href=https://99999.fun/links/ title=链接><span>链接</span></a></li><li><a href=https://99999.fun/about/ title=关于><span>关于</span></a></li><li><a href=https://99999.fun/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://99999.fun/>Home</a>&nbsp;»&nbsp;<a href=https://99999.fun/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">python爬虫学习笔记</h1><div class=post-meta><span title='2021-12-31 01:05:00 +0000 UTC'>2021-12-31</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://99999.fun//posts/python%e7%88%ac%e8%99%ab%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>urllib，xpath，jsonpath，beautiful，requests，selenium，Scrapy</p><p>python库内置的HTTP请求库
urllib.request 请求模块
urllib.error 异常处理模块
urllib.parse url解析模块
urllib.robotparsef robots.txt解析模块</p><p>urllib.request提供了最基本的http请求方法，主要带有处理授权验证，重定向，浏览器Cookies功能</p><p>模拟浏览器发送get请求，就需要使用request对象，在该对象添加http头
import urllib.requst
response = urllib.request.urlopen(&lsquo;<a href="https://xiaochenabc123.test.com/'">https://xiaochenabc123.test.com/'</a>)
print(response.read().decode(&lsquo;utf-8&rsquo;))</p><p>使用type()方法
import urllib.requst
response = urllib.request.urlopen(&lsquo;<a href="https://xiaochenabc123.test.com/'">https://xiaochenabc123.test.com/'</a>)
print(type(response))</p><p>HTTPResposne类型对象</p><p>通过status属性获取返回的状态码
import urllib.requst
response = urllib.request.urlopen(&lsquo;<a href="https://xiaochenabc123.test.com/'">https://xiaochenabc123.test.com/'</a>)
print(response.status)
print(response.getheaders())</p><p>post发送一个请求，只需要把参数data以bytes类型传入
import urllib.parse
import urllib.request
data = bytes(urllib.parse.urlencode({&lsquo;hallo&rsquo;:&lsquo;python&rsquo;}),encoding=&lsquo;utf-8&rsquo;)
response = urllib.request.urlopen(&lsquo;<a href="http://httpbin.org/post'.data">http://httpbin.org/post'.data</a> = data)
print(response.read())</p><p>timeout参数用于设置超时时间，单位为秒
import urllib.request
response = urllib.request.urlopen(&lsquo;<a href="https://xiaochenabc123.test.com/',timeout=1">https://xiaochenabc123.test.com/',timeout=1</a>)</p><p>这里设置超时时间为1秒，如果超了1秒，服务器依然没有响应就抛出URLError异常，可以结合try和except</p><pre><code>import urllib.parse
import urllib.request
url = &quot;https://xiaochenabc123.test.com/&quot;
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'}
data = urllib.request.Request(url=url, headers=headers)
hi = hallo = urllib.request.urlopen(data)
#hallo = urllib.request.urlopen(data).read().decode('utf-8')
#print(hallo)

# 6个方法
#abc = hi.read(6) #按字节返回
#xyz = hi.readline()  # 读取一行
#a = hi.readlines()  # 一行一行的读取
#b = hi.getcode()  # 状态码
#c = hi.geturl()  # 获取url地址
#d = hi.getheaders()  # 获取状态信息

# 下载到本地

#urllib.request.urlretrieve(url,data.html)

#url_img = &quot;https://xiaochenabc123.test.com/1.jpg&quot;
#urllib.request.urlretrieve(url_img,abc.jpg)

#url_mp4 = &quot;https://xiaochenabc123.test.com/1.mp4&quot;
#urllib.request.urlretrieve(url_img,xyz.mp4)


# auote() 转Unicode编码
#name = urllib.parse.quote(&quot;哈哈哈&quot;)

#urlencode(),get拼接
gg = {
    &quot;name&quot;: &quot;小陈&quot;,
    &quot;data&quot;: &quot;hallo word&quot;
}
haha = urllib.parse.urlencode(gg)
#print(haha)

# post请求
ssr = {
    data: &quot;abc&quot;
}
abcxyz = urllib.parse.urlencode(ssr).encode('utf‐8')
nogo = urllib.request.Request(url=url, headers=headers, data=abcxyz)
yesgo = urllib.request.urlopen(nogo)
#print(yesgo.read().decode('utf‐8'))

#ajax请求（get和post）


#cookie（扩展headers）

headers = {
    'Cookie': 'xxxx'
}




#HTTPHandler()

#request = urllib.request.Request(url=url, headers=headers) 
#handler = urllib.request.HTTPHandler()
#opener = urllib.request.build_opener(handler) 
#response = opener.open(request) 
#print(response.read().decode('utf‐8'))



#代理服务器
urls = &quot;http://baidu.com&quot;

request = urllib.request.Request(url=urls, headers=headers)
proxies = {'https': '14.115.106.223:808'}
handler = urllib.request.ProxyHandler(proxies=proxies) 
opener = urllib.request.build_opener(handler) 
response = opener.open(request) 
content = response.read().decode('utf‐8')
#print(content)
</code></pre><hr><pre><code>from lxml import etree
import urllib.request

# 解析本地文件
#html_data = etree.parse('./hallo.html', etree.HTMLParser())
# 解析服务器响应文件
# html = etree.HTML(response.read().decode('utf‐8')
#result = etree.tostring(html_data)
# print(result.decode('utf-8'))
# /为查找子节点，//为查找全部子孙节点（不考虑层级）
# list = html_data.xpath(&quot;//ul/li&quot;)
# 查找全部带id
# list = html_data.xpath(&quot;//ul/li[@id]/text()&quot;)
# 查找指定id
# list = html_data.xpath('//ul/li[@id=&quot;a&quot;]/text()')
# 查找指定class的
# list = html_data.xpath('//ul/li[@class=&quot;a&quot;]/text()')
# 模糊查询（包含）
# list = html_data.xpath('//ul/li[contains(@id, &quot;ab&quot;)]/text()')
# 查找以什么开头的
# list = html_data.xpath('//li[starts-with(@id, &quot;a&quot;)]/text()')
# 查找以什么结尾的
# list = html_data.xpath('//li[ends-with(@id, &quot;a&quot;)]/text()')
# 查找内容
# list = html_data.xpath('//a[text()=&quot;小陈的辣鸡屋&quot;]/text()')
# 多属性匹配(和)
# list = html_data.xpath('//li[contains(@id, &quot;a&quot;) and @href=&quot;https://xiaochenabc123.test.com&quot;]/a/text()')
# 或者
# list = html_data.xpath('//li[contains(@id, &quot;a&quot;) | @href=&quot;https://xiaochenabc123.test.com&quot;]/a/text()')
# 查找指定顺序的
# list = html_data.xpath('//a[1]/text()') #获取第一个，可以指定第几个
# list = html_data.xpath('//a[last()]/text()') #获取最后一个
# list = html_data.xpath('//a[position()&lt;6]/text()') # 获取前5个
# list = html_data.xpath('//a[last()-3]/text()') #获取倒数第4个
# print(list)

#实例：获取logo和logo的url
#url = &quot;https://xiaochenabc123.test.com&quot;
#headers = {
#    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'
#}
#request = urllib.request.Request(url=url, headers=headers)
#response = urllib.request.urlopen(request)
#content = response.read().decode(&quot;utf-8&quot;)
#hallo = etree.HTML(content)
#data = hallo.xpath('//h3[@id=&quot;logo&quot;]/a/@href')[0]
#data1 = hallo.xpath('//h3[@id=&quot;logo&quot;]/a/text()')[0]
#print(data,data1)


#获取文章的标题




#headers = {
#    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'
#}
#
#if __name__ == &quot;__main__&quot;:
#    page1 = int(input(&quot;开始&quot;))
#    page2 = int(input(&quot;结束&quot;))
#
#    for page in range(page1, page2 + 1):
#        urldata = &quot;https://xiaochenabc123.test.com/archives/&quot; + str(page) + &quot;.html&quot;
#        #print(urldata)
#        request = urllib.request.Request(url=urldata, headers=headers)
#        response = urllib.request.urlopen(request)
#        if (response.getcode() == 200):
#            content = response.read().decode(&quot;utf-8&quot;)
#            hallo = etree.HTML(content)
#            data = hallo.xpath('//h3[@class=&quot;post-title&quot;]/a/text()')[0]
#            print(data)
#        elif(response.getcode() == 404):
#            continue


#爬取网站的全部a链接，并且返回到数组中

#if __name__ == &quot;__main__&quot;:
#    url = input(&quot;请输入要爬取的url&quot;)
#    abc = int(input(&quot;是否启动只搜索当前域名的url,请回复0或者1&quot;))
#    headers = {
#       'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'
#    }
#    request = urllib.request.Request(url=url, headers=headers)
#    response = urllib.request.urlopen(request)
#    content = response.read().decode(&quot;utf-8&quot;)
#    hallo = etree.HTML(content)
#    data = hallo.xpath('//a/@href')
#    a = 1
#    b = 1
#    c = &quot;https&quot;
#    d = &quot;http&quot;
#    if (abc == 1):
#        while a &lt; len(data):
#            if (c or d in data[a]):
#                if (url in data[a]):
#                    urldata = data[a]
#                    print(urldata)
#                a += 1
#    else:
#        while b &lt; len(data):
#            if (c or d  in data[a]):
#                urldata = data[b]
#                print(urldata)
#                b += 1
</code></pre><hr><p>练习（json解析bing官方提供的api，获取精密壁纸）</p><pre><code>import jsonpath
import json
import urllib.request
# pip install jsonpath
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'
}
abc = 0
while abc == 0 :
    a = int(input(&quot;请输入要下载的张数（bing最多只能下载8张）&quot;))
    aa = str(a)
    if(a&lt;=8):
        urlapi = &quot;https://cn.bing.com/HPImageArchive.aspx?format=js&amp;idx=0&amp;n=&quot;+aa+&quot;&amp;mkt=zh-CN&quot;
        request = urllib.request.Request(url=urlapi, headers=headers)
        resp = urllib.request.urlopen(request)
        content = resp.read().decode(&quot;utf-8&quot;)
        with open(&quot;data.json&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as fp:
            fp.write(content)
        obj = json.load(open(&quot;data.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;))
        hallo = jsonpath.jsonpath(obj, &quot;$..url&quot;)
        img = jsonpath.jsonpath(obj, &quot;$..enddate&quot;)
        c = 0
        b = 0
        while c &lt; len(hallo):
            hi = &quot;https://cn.bing.com&quot;
            data = hi + hallo[c]
            print(data)
            c+=1
            while b &lt; len(img):
                haha = img[b] + &quot;.jpg&quot;
                print(haha)
                urllib.request.urlretrieve(data, filename=haha)
                b+=1
                break
        print(&quot;下载完成！！！&quot;)
        abc = 1
    else:
        print(&quot;您输入的张数超过8张，bing官方api只能下载8张，请重新输入&quot;)
</code></pre><hr><p>Beautiful Soup解析库，和lxml一样，可以从html或者xml中解析数据和提取数据，Beautiful Soup会自动将输入数据转换为unicode编码，输出数据转换为utf-8编码。不需要考虑编码问题。除非文档没有说明编码</p><p>目前使用的是Beautiful Soup 4，简称bs4，Beautiful Soup3已停止维护</p><p>安装</p><p>pip install beautifulsoup4</p><p>注意：Beautiful Soup支持第三方解析器，如果不使用第三方解析器的话，将使用Python标准库中的html解析器</p><pre><code>BeautifulSoup(html, html.parser) # 内置标准库
BeautifulSoup(html, lxml) # lxml库，需要下载lxml库
BeautifulSoup(html, xml) # lxml的xml解析，需要下载lxml库
BeautifulSoup(html, html5lib) # html5lib，需要下载html5lib库 pip install html5lib
</code></pre><p>第一个例子</p><pre><code>from bs4 import BeautifulSoup
# data =  BeautifulSoup(open(&quot;haha.html&quot;),encoding=&quot;utf-8&quot;, &quot;lxml&quot;)
data =  BeautifulSoup(open(&quot;haha.html&quot;))
print(data.prettify())
</code></pre><p>bs会将html文档解析为树状结构，该树状结构的节点是Python对象，而这些对象可以分为4种：</p><p>Tag：标签，通过tag获取指定标签内容，print(data.div)，可以通过data.标签名的方式获取标签的内容（注意：输出第一个符合条件的标签）</p><p>检查对象的类型：print (type(data.div))，可以看到输出结果为&lt;class &lsquo;bs4.element.Tag&rsquo;>，说明该对象为tag</p><p>tag有两个属性，分别为name和attrs</p><pre><code>print (data.name)
print (data.div.name)
</code></pre><p>可以看到输出结果为[document]和div，data的name为[document]，而标签输出为标签本身的名字</p><pre><code>print (data.div.attrs)
</code></pre><p>可以看到输出结果是{&lsquo;class&rsquo;: [&lsquo;xxx&rsquo;]}的键值对，可以通过data.div[&ldquo;xxx&rdquo;]方式获取属性值，也可以修改属性值（data.div[&ldquo;class&rdquo;]= &ldquo;nav&rdquo;），删除属性（del data.div[&ldquo;class&rdquo;]）</p><p>NavigableString：标签内部的文本（.string），print (data.p.string)，判断类型，print (type(data.p.string))，输出结果为&lt;class &lsquo;bs4.element.NavigableString&rsquo;></p><p>BeautifulSoup：文档的全部内容，也就是data本身，print (type(data))，&lt;class &lsquo;bs4.BeautifulSoup&rsquo;></p><p>Comment：特殊一点的NavigableString类型，可以输出注释的内容（不带注释符号），通过类型判断print (type(data.span.string))，可以看到输出&lt;class &lsquo;bs4.element.Comment&rsquo;></p><p>查找</p><pre><code>print(data.div.content) # 查找div的全部子节点，并且以列表的方式输出，可以使用索引来指定获取第几个节点

print(data.div.children)  #一样是查找div的全部子节点，不过是list生成器，需要手动遍历获取

print(data.select(&quot;span&quot;)) #查找标签，也可以通过类名或者id名查找，支持组合查找（和css方式一样）

print(data.find(&quot;div&quot;))

print(data.find(&quot;div&quot;,class_=&quot;nav&quot;))
</code></pre><hr><p>selenium是一个web应用自动化测试工具，selenium可以直接运行在目前主流浏览器驱动（本身没有浏览器功能，需要搭配第三方浏览器，支持无界面浏览器），模拟用户操作</p><p>因为其自动化和可以直接运行在浏览器的原因，可以用于爬虫</p><p>chromedriver：https://npm.taobao.org/mirrors/chromedriver（淘宝镜像地址，下载的驱动版本要和浏览器内核的版本一样）
edge: <a href=https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/>https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/</a>
（webdriver.Edge()）
火狐：https://github.com/mozilla/geckodriver/releases （webdriver.Firefox()）</p><p>安装</p><p>pip install selenium</p><p>导入驱动</p><pre><code>from selenium import webdriver
path = &quot;浏览器驱动路径&quot;
driver = webdriver.Chrome(path)
url = &quot;https://xiaochenabc123.test.com&quot;
driver.get(url)
data = driver.page_source
print(data)
</code></pre><p>元素定位（模拟鼠标和键盘操作元素的点击或者输入等等，在操作元素之前需要获取到元素的位置）</p><p>注意：element是返回单个对象，elements可以返回多个对象</p><p>根据id查找元素</p><p>data = driver.find_element_by_id(&ldquo;app&rdquo;)</p><p>根据name属性值查找
data = driver.find_element_by_name("")</p><p>根据xpath查找
data = driver.find_elements_by_xpath()</p><p>根据标签名查找
data = driver.find_elements_by_tag_name()</p><p>使用bs4的方式查找
data = driver.find_element_by_css_selector("#app")</p><p>根据连接的文本查找
data = driver.find_element_by_link_text()</p><p>根据连接的文本查找（模糊）
data = driver.find_element_by_partial_link_text()</p><p>根据class名查找
data = driver.find_element_by_class_name()</p><p>交互和数据获取</p><p>数据获取</p><p>获取属性值</p><p>print(data.get_attribute(&ldquo;href&rdquo;))</p><p>获取标签名</p><p>print(data.tag_name)</p><p>获取元素文本</p><p>print(data.text)</p><p>交互</p><p>指定浏览器大小</p><p>driver.set_window_size(900, 1000)</p><p>浏览器的前进和后退，以及刷新</p><p>driver.forward()
driver.back()
driver.refresh()</p><p>input输入框操作</p><p>data.click() #单击元素
data.send_keys(&ldquo;hallo&rdquo;) #输入
data.clear() #清除内容</p><p>执行指定js脚本</p><p>driver.execute_script(js)</p><p>例如：</p><pre><code>import time
from selenium import webdriver
path = &quot;msedgedriver.exe&quot;
driver = webdriver.Edge(path)
url = &quot;https://www.baidu.com/&quot;
driver.get(url)
data = driver.page_source
datas = driver.find_element_by_id(&quot;kw&quot;)
datas.send_keys(&quot;小陈的辣鸡屋&quot;)
a = driver.find_element_by_id(&quot;su&quot;)
a.click()
time.sleep(2)
#bottom = 'document.documentElement.scrollTop=10000'
#driver.execute_script(bottom)
b = driver.find_element_by_link_text(&quot;小陈的辣鸡屋&quot;)
b.click()
</code></pre><p>Phantomjs是一个无界面浏览器，因为其不需要进行gui渲染，效率比较高，但是已经停止更新，这里只了解一下，推荐使用Headless Chrome</p><p><a href=https://phantomjs.org/>https://phantomjs.org/</a></p><p>phantomjs操作方式一样</p><pre><code>driver = webdriver.PhantomJS(path)
</code></pre><p>因为没有界面，访问网页是没有界面的，需要通过快照获取，driver.save_screenshot(&ldquo;xiaochenabc123.test.com.jpg&rdquo;)</p><p>Headless Chrome是基于Chrome 59版本以及以上版本的无界面模式（mac和Linux最低要求59版本，而win需要60版本）</p><pre><code>from selenium import webdriver
from selenium.webdriver.chrome.options import Options
chrome_options = Options()
chrome_options.add_argument('--headless') # 无界面模式启动
chrome_options.add_argument('--disable-gpu')
path = &quot;Chrome.exe&quot; # Chrome浏览器的路径
chrome_options.binary_location = path
driver = webdriver.Chrome(chrome_options=chrome_options) # 实例化
url = &quot;https://xiaochenabc123.test.com&quot;
driver.get(&quot;url&quot;)
driver.save_screenshot(&quot;xiaochenabc123.test.com.jpg&quot;)
</code></pre><p>具体操作方法和selenium一样</p><hr><p>requests库是基于Python开发并且封装的http库，HTTP for Humans</p><p>安装</p><p>pip install requests</p><p>使用方式很简单</p><pre><code>import requests
url = &quot;https://xiaochenabc123.test.com&quot;
response = requests.get(url)
response.status_code # 200 返回状态码
print(type(response)) # 返回类型
response.encoding = &quot;utf-8&quot; # 响应的编码
print(response.text) # 返回源代码
print(response.url) # 返回请求的url
print(response.content) # 返回二进制数据
print(response.headers) # 返回响应头
</code></pre><p>requests在请求json的时候还可以直接获取</p><p>get请求</p><p>get请求的参数通过params属性传递，不需要考虑url的编码问题，不需要考虑请求对象，例如：</p><pre><code>data = {
    'wd': 'python'
}
headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'
}
response = requests.get('https://baidu.com/s?', params=data, headers=headers)
print (response.text)
</code></pre><p>post请求</p><p>post请求不需要考虑编解码，不需要考虑请求对象</p><pre><code>data = {
    'url': 'xiaochenabc123.test.com',
    'name': 'root'
}
url = &quot;https://httpbin.org/post&quot;
response = requests.post(url=url, data=data, headers=headers)
import json
obj = json.loads(response.text,encoding=&quot;utf-8&quot;)
print (obj)
</code></pre><p>注意：默认使用application/x-www-form-urlencoded进行编码，如果想传递json，需要使用requests.post()的json参数，上传文件用files参数</p><p>另外还支持put()和delete()</p><p>传入cookie需要cookies参数，支持设置请求超时（timeout参数，单位为秒）</p><p>代理用proxies参数，指向一个字典（代理池）</p><hr><p>scrapy是基于Python开发的爬取数据，提取数据的框架，可应用在数据挖掘，数据存储</p><p>安装</p><p>pip install scrapy</p><p>scrapy架构组件分为Scrapy Engine（引擎），Scheduler（调度器），Downloader（下载器），Spider（爬虫），Item Pipeline（数据管道），Downloader middlewares（下载中间件），Spider middlewares（Spider中间件）</p><p>Scrapy Engine：负责控制数据流（Data Flow）在组件中的流动，例如通信，数据传递，在某些情况下还触发相对应的事件</p><p>Scheduler：负责接收引擎发送的request请求并且将其存储在调度器中，当引擎需要时提供给引擎，调度器会自动去重url（也可以不去重）</p><p>Downloader：负责下载引擎发送的全部request请求的响应数据，将获取到数据提供给Spider</p><p>Spider：负责处理全部响应数据，并且进行分析提取数据，获取需要的数据，并且将需要进行额外跟进的url传递给引擎，以便重新进入调度器，一般情况一个Spider只负责一个或者多个特定的url</p><p>Item Pipeline：负责处理Spider提取出来的数据，经过分析，过滤，存储等步骤，最后存储在本地或者数据库中</p><p>Downloader middlewares：其是引擎和Downloader之间的钩子，自定义扩展下载器的功能，例如更换ua，ip等等</p><p>Spider middlewares：Spider和引擎之间的钩子，用来处理Spider的输入响应和输出数据，自定义扩展Spider的功能</p><p>具体数据流线路：引擎请求一个url，并且获取到处理这个url的Spider，并且向该Spider请求要第一个爬取的url，引擎获取到要爬取的url，将其传递给调度器，引擎再向调度器请求要下一个爬取的URL，调度器返回url给引擎，引擎通过下载中间件传递给下载器，由下载器下载数据，当数据下载完毕（不管是否成功），下载器将响应数据通过下载中间件返回给引擎，引擎得到数据并且通过Spider中间件传递给Spider处理数据，Spider处理数据并且将处理完毕的数据以及需要跟进的url提供给引擎，引擎再将处理完毕的数据传递给Item Pipeline进行下一步处理，并且将需要跟进的url传递给调度器，一直重复循环直到调度器中没有request</p><p>新建项目：</p><p>scrapy startproject scrapyDemo</p><p>这个项目中包含了一些文件，分别是scrapy.cfg（配置文件），scrapyDemo/（项目的Python模块，将在这里编写程序），items.py（item文件，目标文件），pipelines.py（管道文件，用来处理数据，默认为300优先级，值越小优先级越高），settings.py（项目的设置文件，例如robots协议是否遵守，定义ua等等），spiders/（spider爬虫程序存放的目录）</p><p>spider爬虫程序（/spiders目录下）</p><p>创建一个爬虫必须继承scrapy.Spider类，并且定义三个参数（必须唯一，用来区别其他爬虫），start_urls（爬虫程序执行时要爬取的url），parse()在被调用时，下载完成后得到的响应将作为唯一参数传递给该方法，这个方法负责解析返回回来的数据，以及提取数据和后续要进行下一步处理的响应数据</p><p>快速创建一个基础scrapy爬虫程序</p><p>scrapy genspider cjlio xiaochenabc123.test.com</p><pre><code>import scrapy

class CjlioSpider(scrapy.Spider):
    name = 'cjlio'
    allowed_domains = ['xiaochenabc123.test.com'] # 过滤爬取的URL，不在此范围内的域名会被过滤掉
    start_urls = ['http://xiaochenabc123.test.com/']

    def parse(self, response):
        content = response.text
        print(content)
</code></pre><p>response属性：.text（返回响应的字符串），.body（返回二进制数据），.xpath()（使用xpath方法来解析，返回的数据为列表），.extract()获取seletor对象的data属性值，.extract_first()获取seletor列表的第一个的数据</p><p>还有.url（url地址），.status（状态码），.encoding（响应正文的编码），.request（生成request对象），.css()（用css选择器方式解析数据），.urljoin()（生成绝对url）</p><p>执行爬虫程序（cjlio为爬虫的name）</p><p>scrapy crawl cjlio</p><p>将获取的数据存储为json文件</p><p>scrapy crawl cjlio -o cjlio.json</p><p>注意：因为scrapy默认使用ascii存储json，需要改为utf-8</p><p>在settings.py中添加FEED_EXPORT_ENCODING = &lsquo;utf-8&rsquo;解决</p><p>关闭robots协议（不遵守robots协议，注释掉或者改为False，默认遵守robots协议）</p><pre><code>ROBOTSTXT_OBEY = False
</code></pre><p>scrapy shell是一个交互终端，在没有启动spider的情况下尝试以及测试程序的xpath和css表达式</p><p>scrapy shell可以借助ipython终端</p><p>pip install ipython</p><p>注意：如果安装ipython，scrapy将使用ipython代替Python标准终端，ipython提供了自动补全，高亮等等功能</p><p>调试例子（不需要进入Python环境，可以直接在终端中调试）</p><p>scrapy shell xiaochenabc123.test.com</p><p>response.text</p><p>response.xpath()</p><p>items.py定义数据结构</p><pre><code>import scrapy
class ScrapydemoItem(scrapy.Item):
    name = scrapy.Field()
    img = scrapy.Field()
</code></pre><p>cjlio.py（爬虫程序）</p><pre><code>import scrapy
class CjlioSpider(scrapy.Spider):
    name = 'cjlio'
    allowed_domains = ['xiaochenabc123.test.com']
    start_urls = ['http://xiaochenabc123.test.com/']
    def parse(self, response):
        content = response.text
        name = content.xpath(&quot;xpath匹配&quot;).extract_first()
        img = content.xpath(&quot;xpath匹配&quot;).extract_first()
        print(name, img)
</code></pre><p>管道封装</p><p>cjlio.py（爬虫程序）</p><pre><code>from scrapyDemo.items import ScrapydemoItem
...
data = scrapyDemoItems(name=name,img=img)
yield data # 每获取一次data数据，就是返回一次数据给管道
# yield的作用类似于生成器（generator）
</code></pre><p>注意：如果想使用管道，需要在settings.py中开启</p><p>找到</p><pre><code>ITEM_PIPELINES = {
    'scrapyDemo.pipelines.ScrapydemoPipeline': 300,
}
</code></pre><p>将注释去掉，300指优先级（优先级1到1000），值越小优先级越高</p><p>pipelines.py（管道文件）中的item就是爬虫程序返回的data数据</p><p>启动爬虫之前（open_spider(self, spider)）</p><p>启动爬虫之后（close_spider(self, spider)）</p><p>开启多管道</p><p>在 ITEM_PIPELINES 字段中添加新管道（settings.py）</p><p>然后在pipelines.py定义新管道的类</p><p>查询当前项目中的爬虫任务：scrapy list</p><p>自定义UA</p><p>settings.py文件中USER_AGENT字段或者DEFAULT_REQUEST_HEADERS都可以</p><p>项目中配置headers</p><p>spiders爬虫程序</p><pre><code>headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'
}
url = &quot;https://xiaochenabc123.test.com&quot;
yield scrapy.Request(
    url=url,
    headers=headers
)
</code></pre><p>数据入库</p><p>安装pymysql</p><p>pip install pymysql</p><p>settings.py</p><p>（pymysql）
DB_HOST = &ldquo;127.0.0.1&rdquo;
DB_RORT = 3306
DB_USER = &ldquo;root&rdquo;
DB_PASSWORD = &ldquo;123abc&rdquo;
DB_NAME = &ldquo;dataMax&rdquo;
DB_CHARSET = &ldquo;utf8&rdquo;</p><p>开一条新管道，专门处理数据入数据库</p><p>pipeline.py</p><pre><code>from scrapy.utils.project import get_project_settings
import pymysql
#读取settings文件
class MysqlMax:
    def open_spider(self, spider):
        settings = get_project_settings()
        self.host = settings['DB_HOST']
        self.port = settings['DB_PORT']
        self.user = settings['DB_USER']
        self.password = settings['DB_PASSWORD']
        self.name = settings['DB_NAME']
        self.charset = settings['DB_CHARSET']
        self.connect()
    def connect(self):
        self.conn = pymysql.connect(
            host = self.host,
            port = self.port,
            user = self.user,
            password = self.password,
            name = self.name,
            charset = self.charset
        )
        self.cursor = self.conn.cursor()

    def process_item(self,item,spider):
        sql = 'insert into book(name,src) values(&quot;{}&quot;,&quot;{}&quot;).format(item['name'],item['img'])
        self.cursor.execute(sql)
        self.conn.commit()
        # 执行sql语句，并且提交
        return item

     def close_spider(self, spider):
         self.cursor.close()
         self.conn.close()
</code></pre><p>链接跟进和链接提取器</p><p>爬虫程序文件</p><pre><code>rules = (
    Rule(LinkExtractor(allow = &quot;&quot;),
        callback = &quot;parse_item&quot;, 
        follow = True),
)
</code></pre><p>allow为正则表达式，当链接满足正则表达式提取，为空则全部匹配（链接提取器）</p><p>callback为指定解析数据的规则</p><p>follow一般情况默认为False，指定是否从response提取的链接进行跟进，当callback为none时，follow为True</p><p>还有dent =()，用来过滤符合正则表达式的链接，当符合时不提取</p><p>allow_domains：允许的域名，deny_domains：不允许的域名</p><p>restrict_xpaths：提取符合xpath的链接，restrict_css：提取符合选择器的链接</p><p>注意：follow当为True会一直提取符合规则的链接，直到全部链接提取完毕</p><p>日志以及日志等级</p><p>日志等级</p><p>CRITICAL：严重错误
ERROR：一般错误
WARNING：警告
INFO：一般信息
DEBUG：调试信息（默认，只有出现DEBUG以及以上等级的日志才会打印输出）</p><p>settings.py文件可以设置哪些日志显示，哪些不显示</p><p>LOG_FILE：将信息存储到文件中，不显示在输出界面，后缀为.log</p><p>LOG_LEVEL：指定日志显示的等级</p><p>例如：
LOG_LEVEL = &ldquo;WARNING&rdquo;
LOG_FILE= &ldquo;demo.log&rdquo;</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://99999.fun/tags/python/>Python</a></li></ul><nav class=paginav><a class=prev href=https://99999.fun/posts/128/><span class=title>« Prev</span><br><span>Golang进阶扩展笔记</span>
</a><a class=next href=https://99999.fun/posts/124/><span class=title>Next »</span><br><span>Pinia---vuejs的轻量级状态管理库</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://99999.fun/>知政的个人博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>